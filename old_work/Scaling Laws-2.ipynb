{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d517354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/op/miniconda3/envs/p39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d6b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights(ntasks, alpha):\n",
    "    # Calculate the unnormalized weights for each task\n",
    "    unnormalized_weights = [i ** (-alpha) for i in range(1, ntasks + 1)]\n",
    "    \n",
    "    # Calculate the normalization factor (sum of unnormalized weights)\n",
    "    normalization_factor = sum(unnormalized_weights)\n",
    "    \n",
    "    # Normalize the weights so that they sum to 1\n",
    "    normalized_weights = [weight / normalization_factor for weight in unnormalized_weights]\n",
    "    \n",
    "    return normalized_weights\n",
    "\n",
    "def generate_multitask_sparse_parity(n, k, ntasks, num_samples, control_bit_probs=None):\n",
    "    # Generate ntasks random subsets Si of k indices from {1,2,...,n}\n",
    "    subsets = [np.random.choice(range(n), k, replace=False) for _ in range(ntasks)]\n",
    "\n",
    "    # Set uniform distribution if no control_bit_probs provided\n",
    "    if control_bit_probs is None:\n",
    "        control_bit_probs = [1/ntasks] * ntasks\n",
    "\n",
    "    # Generate random dataset\n",
    "    task_bits = np.random.randint(2, size=(num_samples, n))\n",
    "    task_nums = np.random.choice(ntasks, size=num_samples, p=control_bit_probs)\n",
    "    control_bits = np.zeros((num_samples, ntasks), dtype=int)\n",
    "    control_bits[np.arange(num_samples), task_nums] = 1\n",
    "\n",
    "    # Compute the sparse parity for the active task\n",
    "    sparse_parities = np.array([np.sum(task_bits[i, subsets[task_num]]) % 2 for i, task_num in enumerate(task_nums)])\n",
    "\n",
    "    # Combine control bits, task bits, and sparse parity as input-output pairs\n",
    "    input_bits = np.hstack((control_bits, task_bits))\n",
    "\n",
    "    return input_bits, sparse_parities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29c86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "ntasks = 500\n",
    "k = 3\n",
    "alpha = 1.4\n",
    "probs = generate_weights(ntasks,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1d75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bab84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class ReLUMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(ReLUMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58d1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_subtasks(model, task_subsets, loss_func, samples_per_task = 50):\n",
    "  n_tasks = task_subsets.shape[0]\n",
    "  n = task_subsets.shape[1]\n",
    "\n",
    "  task_bits = np.random.randint(2, size = (samples_per_task * n_tasks, n))\n",
    "  output_bits = np.zeros(samples_per_task * n_tasks, dtype = int)\n",
    "  # create outputs for every subtask\n",
    "  for i in range(n_tasks):\n",
    "    input_slice = task_bits[i * samples_per_task : (i + 1) * samples_per_task]\n",
    "    outputs = np.sum(input_slice * task_subsets[i], axis = 1) % 2\n",
    "    output_bits[i * samples_per_task:(i + 1) * samples_per_task] = outputs\n",
    "  input_bits = np.eye(n_tasks)[np.repeat(np.arange(n_tasks), samples_per_task)]\n",
    "  # print(input_bits.shape)\n",
    "\n",
    "  total_input = np.concatenate([input_bits, task_bits], axis = 1)\n",
    "  # calculate the loss for each (in bits)\n",
    "  mod_outputs = model(torch.from_numpy(total_input).float().to(device))\n",
    "  loss = loss_func(mod_outputs, torch.from_numpy(output_bits).long().to(device))\n",
    "  # print(loss.shape)\n",
    "  # return loss in bits for every subtask\n",
    "  loss = loss.cpu()\n",
    "\n",
    "  losses = loss.reshape(-1, samples_per_task).mean(axis = 1) / np.log(2)\n",
    "  # print(losses.shape)\n",
    "  return losses.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98dad829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_subsets(num_tasks, num_bits, task_sizes, random_state = 0):\n",
    "  np.random.seed(random_state)\n",
    "  task_subsets = np.zeros((num_tasks, num_bits))\n",
    "  for i in range(num_tasks):\n",
    "    task_subsets[i][np.random.choice(num_bits, size = task_sizes[i], replace = False)] = 1\n",
    "  return task_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f840a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "batch_size = 20000\n",
    "import gc\n",
    "\n",
    "task_subsets = generate_task_subsets(ntasks, n, np.ones(ntasks, int) * 3)\n",
    "loss_func = nn.CrossEntropyLoss(reduction = 'none')\n",
    "\n",
    "def train_online_model(hidden_dim):\n",
    "    # Set up the model, optimizer\n",
    "    input_dim = n + ntasks\n",
    "    output_dim = 2\n",
    "    model = ReLUMLP(input_dim, output_dim, hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "    losses = np.zeros(num_iterations)\n",
    "    subtask_losses_across_training = np.zeros((num_iterations // 1000, ntasks))\n",
    "\n",
    "\n",
    "    # Train the model for num_iterations iterations\n",
    "    for i in tqdm(range(num_iterations), desc=\"Training\"):\n",
    "        # Generate a new dataset for the current iteration\n",
    "        input_data, output_data = generate_multitask_sparse_parity(n, k, ntasks, batch_size, probs)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        input_data_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "        output_data_tensor = torch.tensor(output_data, dtype=torch.long).to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_data_tensor)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, output_data_tensor)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss\n",
    "        losses[i] = loss.item()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Iteration {}: Loss = {}\".format(i, loss.item()))\n",
    "            # eval on subtasks\n",
    "            subtask_losses = eval_subtasks(model, task_subsets, loss_func)\n",
    "            subtask_losses_across_training[i // 1000] = subtask_losses\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return losses, subtask_losses_across_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f0afdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/10000 [00:00<33:03,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.6965048313140869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 1001/10000 [02:23<24:03,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Loss = 0.6931250691413879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2001/10000 [04:49<21:27,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Loss = 0.6931466460227966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 3001/10000 [07:36<19:16,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Loss = 0.6931455135345459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4001/10000 [10:03<16:34,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Loss = 0.6931328773498535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5001/10000 [12:42<14:01,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000: Loss = 0.6931437253952026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 6001/10000 [15:29<15:42,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6000: Loss = 0.6931556463241577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7001/10000 [18:31<14:22,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7000: Loss = 0.6931427717208862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8001/10000 [21:11<05:36,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8000: Loss = 0.693150520324707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9001/10000 [24:01<03:00,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9000: Loss = 0.6931452751159668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [26:36<00:00,  6.27it/s]\n"
     ]
    }
   ],
   "source": [
    "losses_10 = train_online_model(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "494e23625cccbf8ead78a71d9a173a7be7b64c8d7820b84e94a0c75ba48e3129"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
